# ai/service/rag.py
import logging
import io
from typing import List, Tuple, Optional
from sentence_transformers import SentenceTransformer
import numpy as np

from .vector_store import VectorStore
from .rag_enhancer import rag_enhancer
from .hybrid_search import HybridSearch

logger = logging.getLogger(__name__)

# Singleton pour le modÃ¨le d'embedding (Ã©viter chargements multiples)
_embedding_model_cache = None

def get_embedding_model():
    """Retourne le modÃ¨le d'embedding (singleton)"""
    global _embedding_model_cache
    if _embedding_model_cache is None:
        logger.info("Chargement initial du modÃ¨le d'embedding...")
        _embedding_model_cache = SentenceTransformer("all-MiniLM-L6-v2")
        logger.info("âœ… ModÃ¨le d'embedding chargÃ©: all-MiniLM-L6-v2")
    return _embedding_model_cache

class RAGService:
    """
    RAGService : Retrieval-Augmented Generation
    Utilise un index FAISS pour retrouver les documents pertinents
    et un modÃ¨le SentenceTransformer pour gÃ©nÃ©rer des embeddings.
    """
    def __init__(self):
        logger.info("Initialisation du RAGService...")
        self.embedding_model = get_embedding_model()
        self.vector_store = VectorStore(dim=384)
        logger.info("Index FAISS et mÃ©tadonnÃ©es chargÃ©s avec succÃ¨s.")

    # =========================
    # MÃ©thodes publiques
    # =========================
    def ingest(self, texts: List[str], source: str):
        """
        Ajouter des textes/documents Ã  l'index FAISS
        """
        embeddings = self.embed(texts)
        metadata = [{"source": source, "text": txt} for txt in texts]
        self.vector_store.add(embeddings, metadata)
        logger.info(f"{len(texts)} documents ingÃ©rÃ©s dans l'index.")

    def ask(self, query: str, k: int = 5, language: str = None, category: str = None, min_confidence: float = 0.40) -> Tuple[str, str]:
        """
        RÃ©cupÃ©rer une rÃ©ponse pertinente et le contexte
        Filtre par langue et catÃ©gorie si spÃ©cifiÃ©s
        min_confidence: seuil de similaritÃ© (0-1). Plus bas = plus permissif. DÃ©faut 0.40
        
        AMÃ‰LIORÃ‰ avec enrichissement de requÃªte et re-ranking hybride
        """
        # ğŸ”¥ NOUVEAU : Enrichir la question avec synonymes et contexte
        enriched_query = rag_enhancer.enrich_query(query, category)
        logger.info(f"ğŸ“ RequÃªte enrichie: '{enriched_query[:100]}'")
        
        query_vector = self.embed([enriched_query])
        
        # Rechercher plus de rÃ©sultats pour re-ranking
        search_k = k * 3 if (language or category) else k
        results, scores = self.vector_store.search(query_vector, k=search_k, return_scores=True)

        if not results:
            return "Je n'ai pas trouvÃ© d'information sur ce sujet. Pourriez-vous reformuler votre question ?", ""
        
        # Convertir distance L2 en score de similaritÃ© (0-1)
        # Distance L2: 0 = identique, plus grand = plus diffÃ©rent
        # On normalise: similaritÃ© = 1 / (1 + distance)
        similarities = [1.0 / (1.0 + d) for d in scores]
        
        # VÃ©rifier si le meilleur rÃ©sultat dÃ©passe le seuil
        best_similarity = max(similarities) if similarities else 0.0
        logger.info(f"ğŸ“Š Meilleure similaritÃ©: {best_similarity:.3f} (seuil: {min_confidence})")
        
        if best_similarity < min_confidence:
            logger.warning(f"âŒ SimilaritÃ© trop faible ({best_similarity:.3f} < {min_confidence})")
            return "Je ne suis pas sÃ»r de comprendre votre question. Pourriez-vous la reformuler ou choisir un sujet parmi les catÃ©gories disponibles ?", ""

        # Filtrer par langue ET catÃ©gorie si spÃ©cifiÃ©es, en gardant les scores
        # âš ï¸ IMPORTANT: Si category='general', on filtre SEULEMENT par langue (pas de filtre catÃ©gorie)
        if language:
            filtered_results = []
            filtered_scores = []
            for idx, r in enumerate(results):
                source = r.get("source", "")
                lang_match = f"-{language}" in source
                if lang_match:
                    filtered_results.append(r)
                    filtered_scores.append(similarities[idx] if idx < len(similarities) else 0.0)
                if len(filtered_results) >= k:
                    break
            if len(filtered_results) == 0:
                logger.error(f"âŒ Aucun rÃ©sultat pour la langue {language}")
                return "Je n'ai pas trouvÃ© d'information sur ce sujet dans cette langue. Pourriez-vous reformuler votre question ?", ""
            else:
                logger.info(f"âœ… {len(filtered_results)} rÃ©sultats trouvÃ©s pour la langue {language}")
                results = filtered_results
                similarities = filtered_scores
        
        # ğŸ”¥ NOUVEAU : Re-ranking hybride (sÃ©mantique + mots-clÃ©s)
        logger.info(f"ğŸ¯ Re-ranking hybride de {len(results)} rÃ©sultats...")
        results, similarities = HybridSearch.rerank_results(
            query=query,  # Question ORIGINALE (pas enrichie) pour les mots-clÃ©s
            results=results,
            semantic_scores=similarities,
            keyword_weight=0.5  # 50% mots-clÃ©s, 50% sÃ©mantique
        )
        logger.info(f"âœ… Re-ranking terminÃ©. Top score: {similarities[0]:.3f}")
        
        # Prendre les k meilleurs rÃ©sultats aprÃ¨s re-ranking
        results = results[:k]
        similarities = similarities[:k]
        
        # Extraire le texte des rÃ©sultats, fusionner et limiter la rÃ©pÃ©tition
        context_texts = []
        answer_only_texts = []  # SEULEMENT les rÃ©ponses, AUCUNE question
        seen_texts = set()
        
        for r in results:
            txt = r.get("text", "")
            if txt and txt not in seen_texts:
                # Parser pour sÃ©parer question et rÃ©ponse
                if "\n\n" in txt:
                    parts = txt.split("\n\n", 1)
                    if len(parts) >= 2:
                        question_part = parts[0].strip()
                        answer_part = parts[1].strip()
                        
                        # Context complet pour logs
                        context_texts.append(f"Q: {question_part}\nR: {answer_part}")
                        # Mais ENVOYER AU LLM SEULEMENT LA RÃ‰PONSE!
                        answer_only_texts.append(answer_part)
                        seen_texts.add(txt)
                    else:
                        context_texts.append(txt)
                        answer_only_texts.append(txt)
                        seen_texts.add(txt)
                else:
                    # Pas de sÃ©paration Q/R claire
                    context_texts.append(txt)
                    answer_only_texts.append(txt)
                    seen_texts.add(txt)

        # IMPORTANT: Le contexte envoyÃ© au LLM contient SEULEMENT les rÃ©ponses
        # Pas de questions pour Ã©viter confusion!
        context = "\n\n---\n\n".join(answer_only_texts)
        
        # SÃ©lectionner la rÃ©ponse la plus pertinente (premier rÃ©sultat)
        if answer_only_texts:
            most_relevant = answer_only_texts[0]
            return most_relevant.strip(), context
        
        return "Aucune information pertinente trouvÃ©e.", ""

    # =========================
    # Analyse PDF / Documents
    # =========================
    def ingest_pdf(self, pdf_bytes: bytes, source: str):
        """
        Convertir un PDF en texte et ingÃ©rer
        """
        try:
            import fitz  # PyMuPDF  # type: ignore
        except ImportError:
            raise ImportError("PyMuPDF est requis pour ingÃ©rer des PDF: pip install pymupdf")

        doc = fitz.open(stream=pdf_bytes, filetype="pdf")
        texts = [page.get_text() for page in doc]
        self.ingest(texts, source)
        logger.info(f"PDF ingÃ©rÃ© avec {len(texts)} pages.")

    # =========================
    # Analyse Images
    # =========================
    def ingest_image(self, image_bytes: bytes, source: str):
        """
        Analyse sommaire d'image (OCR ou description simple)
        """
        try:
            from PIL import Image  # type: ignore
            import pytesseract  # type: ignore
        except ImportError:
            raise ImportError("PIL et pytesseract sont requis pour ingÃ©rer des images: pip install pillow pytesseract")

        image = Image.open(io.BytesIO(image_bytes))
        text = pytesseract.image_to_string(image)
        if text.strip():
            self.ingest([text], source)
            logger.info(f"Texte extrait de l'image et ingÃ©rÃ©.")
        else:
            logger.warning("Aucun texte dÃ©tectÃ© dans l'image.")

    # =========================
    # Embedding
    # =========================
    def embed(self, texts: List[str]) -> np.ndarray:
        """
        Retourne les embeddings pour une liste de textes
        """
        return self.embedding_model.encode(texts, convert_to_numpy=True)
    
    def _enrich_query(self, query: str, category: str = None) -> str:
        """
        Enrichit la question avec des mots-clÃ©s spÃ©cifiques Ã  la catÃ©gorie
        pour amÃ©liorer la recherche sÃ©mantique.
        N'enrichit QUE si la question est assez longue (> 6 mots) pour Ã©viter la dilution.
        """
        if not category:
            return query
        
        # Ne pas enrichir les courtes questions pour Ã©viter la dilution
        word_count = len(query.split())
        if word_count < 4:  # Questions trÃ¨s courtes: pas d'enrichissement
            return query
            
        # Mapping des catÃ©gories vers des mots-clÃ©s pertinents
        category_keywords = {
            "plantes medicinales": "plante mÃ©dicale santÃ© remÃ¨de",
            "plantesmedicinales": "plante mÃ©dicale santÃ© remÃ¨de",
            "transformation pfnl": "transformation karitÃ© noix produit",
            "transformationpfnl": "transformation karitÃ© noix produit",
            "science pratique - saponification": "savon fabrication soude huile",
            "sciencepratiquesaponification": "savon fabrication soude huile",
            "metiers informels": "mÃ©tier travail informel secteur",
            "metiersinformels": "mÃ©tier travail informel secteur",
            "civisme": "citoyen devoir responsabilitÃ©",
            "spiritualite et traditions": "tradition spirituelle culture",
            "spiritualiteettraditions": "tradition spirituelle culture",
            "developpement personnel": "compÃ©tence dÃ©veloppement objectif",
            "developpementpersonnel": "compÃ©tence dÃ©veloppement objectif",
            "mathematiques pratiques": "calcul mathÃ©matique surface",
            "mathematiquespratiques": "calcul mathÃ©matique surface",
            "general": "",  # Pas d'enrichissement pour general
        }
        
        # Normaliser la catÃ©gorie pour la recherche
        import unicodedata
        normalized_cat = category.lower()
        normalized_cat = unicodedata.normalize('NFD', normalized_cat)
        normalized_cat = ''.join(c for c in normalized_cat if unicodedata.category(c) != 'Mn')
        normalized_cat = normalized_cat.replace(' ', '').replace('&', '').replace('-', '')
        
        keywords = category_keywords.get(normalized_cat, "")
        
        # Ajouter les mots-clÃ©s Ã  la fin de la question
        if keywords and word_count >= 4:
            logger.info(f"ğŸ” Question enrichie: '{query}' + '{keywords}' (catÃ©gorie: {category})")
            return f"{query} {keywords}"
        return query
