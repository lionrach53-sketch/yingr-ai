# ai/service/rag.py
import logging
import io
from typing import List, Tuple, Optional
from sentence_transformers import SentenceTransformer
import numpy as np

from .vector_store import VectorStore
from .rag_enhancer import rag_enhancer
from .hybrid_search import HybridSearch

logger = logging.getLogger(__name__)

# Singleton pour le mod√®le d'embedding (√©viter chargements multiples)
_embedding_model_cache = None

def get_embedding_model():
    """Retourne le mod√®le d'embedding (singleton)"""
    global _embedding_model_cache
    if _embedding_model_cache is None:
        logger.info("Chargement initial du mod√®le d'embedding...")
        _embedding_model_cache = SentenceTransformer("all-MiniLM-L6-v2")
        logger.info("‚úÖ Mod√®le d'embedding charg√©: all-MiniLM-L6-v2")
    return _embedding_model_cache

class RAGService:
    """
    RAGService : Retrieval-Augmented Generation
    Utilise un index FAISS pour retrouver les documents pertinents
    et un mod√®le SentenceTransformer pour g√©n√©rer des embeddings.
    """
    def __init__(self):
        logger.info("Initialisation du RAGService...")
        self.embedding_model = get_embedding_model()
        self.vector_store = VectorStore(dim=384)
        logger.info("Index FAISS et m√©tadonn√©es charg√©s avec succ√®s.")

    # =========================
    # M√©thodes publiques
    # =========================
    def ingest(self, texts: List[str], source: str):
        """
        Ajouter des textes/documents √† l'index FAISS
        """
        embeddings = self.embed(texts)
        metadata = [{"source": source, "text": txt} for txt in texts]
        self.vector_store.add(embeddings, metadata)
        logger.info(f"{len(texts)} documents ing√©r√©s dans l'index.")

    def ask(self, query: str, k: int = 5, language: str = None, category: str = None, min_confidence: float = 0.40) -> Tuple[str, str]:
        """
        R√©cup√©rer une r√©ponse pertinente et le contexte
        Filtre par langue et cat√©gorie si sp√©cifi√©s
        min_confidence: seuil de similarit√© (0-1). Plus bas = plus permissif. D√©faut 0.40
        
        AM√âLIOR√â avec enrichissement de requ√™te et re-ranking hybride
        """
        # üî• NOUVEAU : Enrichir la question avec synonymes et contexte
        enriched_query = rag_enhancer.enrich_query(query, category)
        logger.info(f"üìù Requ√™te enrichie: '{enriched_query[:100]}'")
        
        query_vector = self.embed([enriched_query])
        
        # Rechercher plus de r√©sultats pour re-ranking
        search_k = k * 3 if (language or category) else k
        results, scores = self.vector_store.search(query_vector, k=search_k, return_scores=True)

        if not results:
            return "Je n'ai pas trouv√© d'information sur ce sujet. Pourriez-vous reformuler votre question ?", ""
        
        # Convertir distance L2 en score de similarit√© (0-1)
        # Distance L2: 0 = identique, plus grand = plus diff√©rent
        # On normalise: similarit√© = 1 / (1 + distance)
        similarities = [1.0 / (1.0 + d) for d in scores]
        
        # V√©rifier si le meilleur r√©sultat d√©passe le seuil
        best_similarity = max(similarities) if similarities else 0.0
        logger.info(f"üìä Meilleure similarit√©: {best_similarity:.3f} (seuil: {min_confidence})")
        
        # IMPORTANT: Le LLM est le juge final. Les scores RAG ne bloquent jamais la r√©ponse.
        if best_similarity < min_confidence:
            logger.warning(f"‚ö†Ô∏è Similarit√© faible ({best_similarity:.3f} < {min_confidence}) - CONTEXTE transmis au LLM quand m√™me.")
            # On log seulement, on ne bloque pas la r√©ponse. Le LLM d√©cidera.

        # Filtrer par langue ET cat√©gorie si sp√©cifi√©es, en gardant les scores
        # ‚ö†Ô∏è IMPORTANT: Si category='general', on filtre SEULEMENT par langue (pas de filtre cat√©gorie)
        if language:
            filtered_results = []
            filtered_scores = []
            for idx, r in enumerate(results):
                source = r.get("source", "")
                lang_match = f"-{language}" in source
                if lang_match:
                    filtered_results.append(r)
                    filtered_scores.append(similarities[idx] if idx < len(similarities) else 0.0)
                if len(filtered_results) >= k:
                    break
            if len(filtered_results) == 0:
                logger.error(f"‚ùå Aucun r√©sultat pour la langue {language}")
                return "Je n'ai pas trouv√© d'information sur ce sujet dans cette langue. Pourriez-vous reformuler votre question ?", ""
            else:
                logger.info(f"‚úÖ {len(filtered_results)} r√©sultats trouv√©s pour la langue {language}")
                results = filtered_results
                similarities = filtered_scores
        
        # üî• NOUVEAU : Re-ranking hybride (s√©mantique + mots-cl√©s)
        logger.info(f"üéØ Re-ranking hybride de {len(results)} r√©sultats...")
        results, similarities = HybridSearch.rerank_results(
            query=query,  # Question ORIGINALE (pas enrichie) pour les mots-cl√©s
            results=results,
            semantic_scores=similarities,
            keyword_weight=0.5  # 50% mots-cl√©s, 50% s√©mantique
        )
        logger.info(f"‚úÖ Re-ranking termin√©. Top score: {similarities[0]:.3f}")
        
        # Prendre les k meilleurs r√©sultats apr√®s re-ranking
        results = results[:k]
        similarities = similarities[:k]
        
        # Extraire le texte des r√©sultats, fusionner et limiter la r√©p√©tition
        context_texts = []
        answer_only_texts = []  # SEULEMENT les r√©ponses, AUCUNE question
        seen_texts = set()
        
        for r in results:
            txt = r.get("text", "")
            if txt and txt not in seen_texts:
                # Parser pour s√©parer question et r√©ponse
                if "\n\n" in txt:
                    parts = txt.split("\n\n", 1)
                    if len(parts) >= 2:
                        question_part = parts[0].strip()
                        answer_part = parts[1].strip()
                        
                        # Context complet pour logs
                        context_texts.append(f"Q: {question_part}\nR: {answer_part}")
                        # Mais ENVOYER AU LLM SEULEMENT LA R√âPONSE!
                        answer_only_texts.append(answer_part)
                        seen_texts.add(txt)
                    else:
                        context_texts.append(txt)
                        answer_only_texts.append(txt)
                        seen_texts.add(txt)
                else:
                    # Pas de s√©paration Q/R claire
                    context_texts.append(txt)
                    answer_only_texts.append(txt)
                    seen_texts.add(txt)

        # IMPORTANT: Le contexte envoy√© au LLM contient SEULEMENT les r√©ponses
        # Pas de questions pour √©viter confusion!
        context = "\n\n---\n\n".join(answer_only_texts)
        
        # S√©lectionner la r√©ponse la plus pertinente (premier r√©sultat)
        if answer_only_texts:
            most_relevant = answer_only_texts[0]
            return most_relevant.strip(), context
        
        return "Aucune information pertinente trouv√©e.", ""

    # =========================
    # Analyse PDF / Documents
    # =========================
    def ingest_pdf(self, pdf_bytes: bytes, source: str):
        """
        Convertir un PDF en texte et ing√©rer
        """
        try:
            import fitz  # PyMuPDF  # type: ignore
        except ImportError:
            raise ImportError("PyMuPDF est requis pour ing√©rer des PDF: pip install pymupdf")

        doc = fitz.open(stream=pdf_bytes, filetype="pdf")
        texts = [page.get_text() for page in doc]
        self.ingest(texts, source)
        logger.info(f"PDF ing√©r√© avec {len(texts)} pages.")

    # =========================
    # Analyse Images
    # =========================
    def ingest_image(self, image_bytes: bytes, source: str):
        """
        Analyse sommaire d'image (OCR ou description simple)
        """
        try:
            from PIL import Image  # type: ignore
            import pytesseract  # type: ignore
        except ImportError:
            raise ImportError("PIL et pytesseract sont requis pour ing√©rer des images: pip install pillow pytesseract")

        image = Image.open(io.BytesIO(image_bytes))
        text = pytesseract.image_to_string(image)
        if text.strip():
            self.ingest([text], source)
            logger.info(f"Texte extrait de l'image et ing√©r√©.")
        else:
            logger.warning("Aucun texte d√©tect√© dans l'image.")

    # =========================
    # Embedding
    # =========================
    def embed(self, texts: List[str]) -> np.ndarray:
        """
        Retourne les embeddings pour une liste de textes
        """
        return self.embedding_model.encode(texts, convert_to_numpy=True)
    
    def _enrich_query(self, query: str, category: str = None) -> str:
        """
        Enrichit la question avec des mots-cl√©s sp√©cifiques √† la cat√©gorie
        pour am√©liorer la recherche s√©mantique.
        N'enrichit QUE si la question est assez longue (> 6 mots) pour √©viter la dilution.
        """
        if not category:
            return query
        
        # Ne pas enrichir les courtes questions pour √©viter la dilution
        word_count = len(query.split())
        if word_count < 4:  # Questions tr√®s courtes: pas d'enrichissement
            return query
            
        # Mapping des cat√©gories vers des mots-cl√©s pertinents
        category_keywords = {
            "plantes medicinales": "plante m√©dicale sant√© rem√®de",
            "plantesmedicinales": "plante m√©dicale sant√© rem√®de",
            "transformation pfnl": "transformation karit√© noix produit",
            "transformationpfnl": "transformation karit√© noix produit",
            "science pratique - saponification": "savon fabrication soude huile",
            "sciencepratiquesaponification": "savon fabrication soude huile",
            "metiers informels": "m√©tier travail informel secteur",
            "metiersinformels": "m√©tier travail informel secteur",
            "civisme": "citoyen devoir responsabilit√©",
            "spiritualite et traditions": "tradition spirituelle culture",
            "spiritualiteettraditions": "tradition spirituelle culture",
            "developpement personnel": "comp√©tence d√©veloppement objectif",
            "developpementpersonnel": "comp√©tence d√©veloppement objectif",
            "mathematiques pratiques": "calcul math√©matique surface",
            "mathematiquespratiques": "calcul math√©matique surface",
            "general": "",  # Pas d'enrichissement pour general
        }
        
        # Normaliser la cat√©gorie pour la recherche
        import unicodedata
        normalized_cat = category.lower()
        normalized_cat = unicodedata.normalize('NFD', normalized_cat)
        normalized_cat = ''.join(c for c in normalized_cat if unicodedata.category(c) != 'Mn')
        normalized_cat = normalized_cat.replace(' ', '').replace('&', '').replace('-', '')
        
        keywords = category_keywords.get(normalized_cat, "")
        
        # Ajouter les mots-cl√©s √† la fin de la question
        if keywords and word_count >= 4:
            logger.info(f"üîç Question enrichie: '{query}' + '{keywords}' (cat√©gorie: {category})")
            return f"{query} {keywords}"
        return query
